Training Batch 0 / 1250 - Loss: 0.6202, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3177, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3152, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3145, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3142, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4210, Acc: 0.8750
Training Batch 600 / 1250 - Loss: 0.3452, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3139, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3441, Acc: 0.9688
Training Batch 900 / 1250 - Loss: 0.3450, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3450, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3455, Acc: 0.9688
Training Batch 1200 / 1250 - Loss: 0.3449, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3447, Acc: 0.9688
Validation Batch 100 / 157 - Loss: 0.3760, Acc: 0.9375
Epoch 1 / 10 - Train Loss: 0.3326, Train Acc: 0.9840, Val Loss: 0.3858, Val Acc: 0.9316
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3449, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3137, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3137, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3136, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3136, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4072, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3448, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3136, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3136, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3448, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3448, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3760, Acc: 0.9375
Training Batch 1200 / 1250 - Loss: 0.3448, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3724, Acc: 0.9375
Validation Batch 100 / 157 - Loss: 0.3759, Acc: 0.9375
Epoch 2 / 10 - Train Loss: 0.3289, Train Acc: 0.9844, Val Loss: 0.3838, Val Acc: 0.9328
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3539, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4384, Acc: 0.8750
Training Batch 600 / 1250 - Loss: 0.3715, Acc: 0.9375
Training Batch 700 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3447, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3447, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3759, Acc: 0.9375
Training Batch 1200 / 1250 - Loss: 0.3447, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3446, Acc: 0.9688
Validation Batch 100 / 157 - Loss: 0.3758, Acc: 0.9375
Epoch 3 / 10 - Train Loss: 0.3284, Train Acc: 0.9850, Val Loss: 0.3859, Val Acc: 0.9330
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3447, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4199, Acc: 0.8750
Training Batch 600 / 1250 - Loss: 0.3448, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3135, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 900 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3758, Acc: 0.9375
Training Batch 1200 / 1250 - Loss: 0.3523, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3758, Acc: 0.9375
Validation Batch 100 / 157 - Loss: 0.3873, Acc: 0.9375
Epoch 4 / 10 - Train Loss: 0.3286, Train Acc: 0.9846, Val Loss: 0.3841, Val Acc: 0.9352
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4071, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3134, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3601, Acc: 0.9375
Training Batch 1200 / 1250 - Loss: 0.3583, Acc: 0.9375
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3602, Acc: 0.9688
Validation Batch 100 / 157 - Loss: 0.3809, Acc: 0.9375
Epoch 5 / 10 - Train Loss: 0.3275, Train Acc: 0.9856, Val Loss: 0.3858, Val Acc: 0.9330
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4071, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3605, Acc: 0.9375
Training Batch 700 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3169, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1200 / 1250 - Loss: 0.3133, Acc: 1.0000
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3479, Acc: 0.9688
Validation Batch 100 / 157 - Loss: 0.3758, Acc: 0.9375
Epoch 6 / 10 - Train Loss: 0.3274, Train Acc: 0.9858, Val Loss: 0.3854, Val Acc: 0.9326
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4070, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3450, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3446, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1200 / 1250 - Loss: 0.3445, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3445, Acc: 0.9688
Validation Batch 100 / 157 - Loss: 0.3758, Acc: 0.9375
Epoch 7 / 10 - Train Loss: 0.3264, Train Acc: 0.9868, Val Loss: 0.3851, Val Acc: 0.9340
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4070, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3752, Acc: 0.9375
Training Batch 1200 / 1250 - Loss: 0.3133, Acc: 1.0000
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3580, Acc: 0.9375
Validation Batch 100 / 157 - Loss: 0.3995, Acc: 0.9062
Epoch 8 / 10 - Train Loss: 0.3266, Train Acc: 0.9866, Val Loss: 0.3838, Val Acc: 0.9344
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3757, Acc: 0.9375
Training Batch 100 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4070, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3447, Acc: 0.9688
Training Batch 900 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1200 / 1250 - Loss: 0.3409, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3757, Acc: 0.9375
Validation Batch 100 / 157 - Loss: 0.3758, Acc: 0.9375
Epoch 9 / 10 - Train Loss: 0.3263, Train Acc: 0.9870, Val Loss: 0.3855, Val Acc: 0.9332
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Training Batch 0 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 100 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 200 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 300 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 400 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 500 / 1250 - Loss: 0.4070, Acc: 0.9062
Training Batch 600 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 700 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 800 / 1250 - Loss: 0.3133, Acc: 1.0000
Training Batch 900 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1000 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1100 / 1250 - Loss: 0.3445, Acc: 0.9688
Training Batch 1200 / 1250 - Loss: 0.3445, Acc: 0.9688
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Validation Batch 0 / 157 - Loss: 0.3758, Acc: 0.9375
Validation Batch 100 / 157 - Loss: 0.4167, Acc: 0.8750
Epoch 10 / 10 - Train Loss: 0.3257, Train Acc: 0.9875, Val Loss: 0.3878, Val Acc: 0.9302
Training Took:  8808.673945426941 s
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Accuracy of the model on the test set: 93 %
Final model (10 epochs): Validation Accuracy: 93.0200% validation accuracy and a test accuracy of 93.6200%!